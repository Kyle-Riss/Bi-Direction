"""
Apple Silicon MPS ìµœì í™” ë„êµ¬
MPS ë°±ì—”ë“œì˜ ì„±ëŠ¥ì„ ìµœëŒ€í•œ í™œìš©í•˜ëŠ” ì„¤ì •
"""
import torch
import os
import gc

def optimize_mps_settings():
    """MPS ë°±ì—”ë“œ ìµœì í™” ì„¤ì •"""
    
    if not torch.backends.mps.is_available():
        print("âŒ MPS not available")
        return False
    
    print("ğŸ Apple Silicon MPS Optimization")
    print("=" * 40)
    
    # 1. MPS ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™”
    os.environ['PYTORCH_MPS_LOW_WATERMARK_RATIO'] = '0.0'
    
    # 2. MPS ìºì‹œ ìµœì í™”
    os.environ['PYTORCH_MPS_ALLOCATOR'] = 'native'  # ë„¤ì´í‹°ë¸Œ í• ë‹¹ì ì‚¬ìš©
    
    # 3. ë©”ëª¨ë¦¬ ì••ë°• ìƒí™©ì—ì„œì˜ ìµœì í™”
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:64'  # ì‘ì€ ì²­í¬ë¡œ ë¶„í• 
    
    # 4. MPS ë°±ì—”ë“œ ì„¤ì • í™•ì¸
    print(f"âœ… MPS Available: {torch.backends.mps.is_available()}")
    print(f"âœ… MPS Built: {torch.backends.mps.is_built()}")
    
    # 5. ë©”ëª¨ë¦¬ ì •ë¦¬
    gc.collect()
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()
    
    print("âœ… MPS optimization applied!")
    return True

def get_mps_optimized_dataloader_kwargs():
    """MPSì— ìµœì í™”ëœ DataLoader ì„¤ì •"""
    return {
        'batch_size': 4,           # ì‘ì€ ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ì ˆì•½)
        'num_workers': 0,           # MPSì—ì„œëŠ” ì‹±ê¸€ ìŠ¤ë ˆë“œê°€ ë” ì•ˆì •ì 
        'pin_memory': False,        # MPSì—ì„œëŠ” ì§€ì› ì•ˆë¨
        'persistent_workers': False, # ë©”ëª¨ë¦¬ ì ˆì•½
        'prefetch_factor': 1,       # í”„ë¦¬í˜ì¹˜ ìµœì†Œí™”
    }

def mps_memory_cleanup():
    """MPS ë©”ëª¨ë¦¬ ì •ë¦¬"""
    gc.collect()
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()
    print("ğŸ§¹ MPS memory cleaned")

def get_mps_device():
    """MPS ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    if torch.backends.mps.is_available():
        return torch.device("mps")
    else:
        return torch.device("cpu")

def create_mps_optimized_model(model_class, **kwargs):
    """MPSì— ìµœì í™”ëœ ëª¨ë¸ ìƒì„±"""
    device = get_mps_device()
    model = model_class(**kwargs)
    model = model.to(device)
    
    # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
    mps_memory_cleanup()
    
    return model, device

def mps_training_step(model, images, targets, optimizer, device):
    """MPSì— ìµœì í™”ëœ í•™ìŠµ ìŠ¤í…"""
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    mps_memory_cleanup()
    
    # ë°ì´í„°ë¥¼ MPSë¡œ ì´ë™
    images = images.to(device, non_blocking=False)  # MPSëŠ” non_blocking ì§€ì› ì•ˆë¨
    targets = targets.to(device, non_blocking=False)
    
    # ìˆœì „íŒŒ
    outputs = model(images)
    
    # ì†ì‹¤ ê³„ì‚°
    if targets.shape[0] > 0:
        target_coords = targets[:, 2:4]
        loss = torch.nn.MSELoss()(outputs, target_coords)
    else:
        loss = torch.tensor(0.0, device=device, requires_grad=True)
    
    # ì—­ì „íŒŒ
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    mps_memory_cleanup()
    
    return loss.item()

def check_mps_performance():
    """MPS ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    if not torch.backends.mps.is_available():
        print("âŒ MPS not available for testing")
        return
    
    device = torch.device("mps")
    
    # ê°„ë‹¨í•œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    print("ğŸ§ª Testing MPS performance...")
    
    # í…ŒìŠ¤íŠ¸ í…ì„œ ìƒì„±
    x = torch.randn(100, 1000, device=device)
    y = torch.randn(1000, 100, device=device)
    
    # í–‰ë ¬ ê³±ì…ˆ í…ŒìŠ¤íŠ¸
    import time
    start_time = time.time()
    
    for _ in range(100):
        z = torch.mm(x, y)
    
    end_time = time.time()
    
    print(f"âœ… MPS Matrix multiplication: {end_time - start_time:.4f}s")
    print(f"âœ… Memory usage: {torch.mps.current_allocated_memory() / 1024**2:.1f} MB")

if __name__ == "__main__":
    optimize_mps_settings()
    check_mps_performance()
    
    print("\nğŸ“‹ MPS Optimized DataLoader Settings:")
    kwargs = get_mps_optimized_dataloader_kwargs()
    for key, value in kwargs.items():
        print(f"  {key}: {value}")

