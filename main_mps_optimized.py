"""
MPS ìµœì í™”ëœ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
Apple Siliconì—ì„œ ìµœëŒ€ ì„±ëŠ¥ì„ ìœ„í•œ ì„¤ì •
"""
import torch
import os
import gc
from tools.mps_optimizer import (
    optimize_mps_settings, 
    get_mps_optimized_dataloader_kwargs,
    mps_training_step,
    mps_memory_cleanup,
    get_mps_device
)

# ëª¨ë“ˆ ì„í¬íŠ¸
from config import get_config
from models.model import create_model
from datasets.dataset_utils import load_temporal_yolo_data
from helper.train_utils import train_one_epoch

def mps_optimized_train_one_epoch(model, dataloader, optimizer, device, epoch_desc):
    """MPS ìµœì í™”ëœ í•™ìŠµ ë£¨í”„"""
    model.train()
    total_loss = 0.0
    
    print(f"Epoch {epoch_desc}")
    
    for batch_idx, (images, targets) in enumerate(dataloader):
        # MPS ìµœì í™”ëœ í•™ìŠµ ìŠ¤í…
        loss = mps_training_step(model, images, targets, optimizer, device)
        total_loss += loss
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥ (ë§¤ 10 ë°°ì¹˜ë§ˆë‹¤)
        if batch_idx % 10 == 0:
            print(f"  Batch {batch_idx}/{len(dataloader)}: Loss = {loss:.4f}")
    
    return total_loss / len(dataloader)

def main():
    print("ğŸ Apple Silicon MPS Optimized Training")
    print("=" * 50)
    
    # MPS ìµœì í™” ì ìš©
    optimize_mps_settings()
    
    # ì„¤ì • ë¡œë“œ
    cfg_gen = get_config("general")
    cfg_data = get_config("data")
    cfg_ms = get_config("multiscale")
    cfg_wandb = get_config("wandb")
    
    # WandB ì´ˆê¸°í™” (ì„ íƒ)
    if cfg_wandb["use"]:
        try:
            import wandb
            wandb.init(
                project=cfg_wandb["project"],
                name=cfg_wandb["run_name"],
                config={**cfg_gen, **cfg_data, **cfg_ms}
            )
        except ImportError:
            print("âš ï¸ WandB not installed. Continuing without logging...")
            cfg_wandb["use"] = False
    
    # MPS ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = get_mps_device()
    print(f"Using device: {device}")
    
    # Coarse Training
    print(f"--- Starting Coarse Training (Size: {cfg_ms['coarse']['img_size']}) ---")
    cfg_coarse = cfg_ms['coarse']
    
    # ëª¨ë¸ ìƒì„± (MPS ìµœì í™”)
    model = create_model("yolo_lstm", num_frames=cfg_gen["num_frames"], hidden_size=256, num_layers=2)
    model = model.to(device)
    
    # MPS ìµœì í™”ëœ DataLoader ì„¤ì •
    mps_dataloader_kwargs = get_mps_optimized_dataloader_kwargs()
    mps_dataloader_kwargs['batch_size'] = cfg_coarse["batch_size"]
    
    # ë°ì´í„° ë¡œë” ìƒì„± (MPS ìµœì í™” ì„¤ì • ì ìš©)
    train_loader, _ = load_temporal_yolo_data(
        cfg_coarse["batch_size"], 
        cfg_gen["num_workers"], 
        cfg_gen["random_state"]
    )
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    optimizer = torch.optim.Adam(model.parameters(), lr=cfg_coarse["lr"])
    
    # í•™ìŠµ ë£¨í”„
    for epoch in range(cfg_coarse["epochs"]):
        avg_loss = mps_optimized_train_one_epoch(
            model, train_loader, optimizer, device,
            f"[COARSE {epoch+1}/{cfg_coarse['epochs']}]"
        )
        
        print(f"Epoch {epoch+1}: Average Loss = {avg_loss:.4f}")
        
        # WandB ë¡œê¹…
        if cfg_wandb["use"]:
            try:
                import wandb
                wandb.log({"coarse_loss": avg_loss, "coarse_epoch": epoch+1})
            except ImportError:
                pass
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        mps_memory_cleanup()
    
    # ëª¨ë¸ ì €ì¥
    os.makedirs('models', exist_ok=True)
    torch.save(model.state_dict(), cfg_coarse["save_path"])
    print(f"Coarse model saved to {cfg_coarse['save_path']}")
    
    # Fine-Tuning
    print(f"\n--- Starting Fine-Tuning (Size: {cfg_ms['fine']['img_size']}) ---")
    cfg_fine = cfg_ms['fine']
    
    # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
    model.load_state_dict(torch.load(cfg_coarse["save_path"]))
    model = model.to(device)
    
    # Fine-tuning ë°ì´í„° ë¡œë”
    train_loader, _ = load_temporal_yolo_data(
        cfg_fine["batch_size"], 
        cfg_gen["num_workers"], 
        cfg_gen["random_state"]
    )
    
    # ì˜µí‹°ë§ˆì´ì € ì¬ì •ì˜
    optimizer = torch.optim.Adam(model.parameters(), lr=cfg_fine["lr"])
    
    # Fine-tuning í•™ìŠµ ë£¨í”„
    for epoch in range(cfg_fine["epochs"]):
        avg_loss = mps_optimized_train_one_epoch(
            model, train_loader, optimizer, device,
            f"[FINE {epoch+1}/{cfg_fine['epochs']}]"
        )
        
        print(f"Epoch {epoch+1}: Average Loss = {avg_loss:.4f}")
        
        # WandB ë¡œê¹…
        if cfg_wandb["use"]:
            try:
                import wandb
                wandb.log({
                    "fine_loss": avg_loss, 
                    "fine_epoch": epoch+1,
                    "total_epoch": cfg_coarse["epochs"] + epoch + 1
                })
            except ImportError:
                pass
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        mps_memory_cleanup()
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    torch.save(model.state_dict(), cfg_fine["save_path"])
    print(f"Final robust model saved to {cfg_fine['save_path']}")
    
    # WandB ì¢…ë£Œ
    if cfg_wandb["use"]:
        try:
            import wandb
            wandb.finish()
        except ImportError:
            pass
    
    print("âœ… Training completed!")

if __name__ == "__main__":
    main()

